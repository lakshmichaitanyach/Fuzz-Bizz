{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Based FizzBuzz Function [Software 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fizzbuzz(n):\n",
    "    \n",
    "    # If the given nunmber is either divisible 3 and 5 the code will return \"fizzbuzz\" and if it is divisible by only 3 it will return \"fuzz\" or else if the number is divisible by 5 it will return \"buzz\"\n",
    "    if n % 3 == 0 and n % 5 == 0:\n",
    "        return 'FizzBuzz'\n",
    "    elif n % 3 == 0:\n",
    "        return 'Fizz'\n",
    "    elif n % 5 == 0:\n",
    "        return 'Buzz'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Datasets in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInputCSV(start,end,filename):\n",
    "    \n",
    "    # lists are versatile and can be updated many times and the size is also variable and the input lists take input values and output list give output values\n",
    "    inputData   = []\n",
    "    outputData  = []\n",
    "    \n",
    "    # we need training data to train our machine learning alogorithm and the more is data the more is its accuracy\n",
    "    for i in range(start,end):\n",
    "        inputData.append(i)\n",
    "        outputData.append(fizzbuzz(i))\n",
    "    \n",
    "    # the dataframe represents the given data into tabular form and to convert excel based analysis to python scripting\n",
    "    dataset = {}\n",
    "    dataset[\"input\"]  = inputData\n",
    "    dataset[\"label\"] = outputData\n",
    "    \n",
    "    # Writing to csv\n",
    "    pd.DataFrame(dataset).to_csv(filename)\n",
    "    \n",
    "    print(filename, \"Created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Input and Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(dataset):\n",
    "    \n",
    "    # we need to process the achieving better results from the applied model in Machine Learning projects the format of the data has to be in a proper manner and data set should be formatted in such a way that more than one Machine Learning and Deep Learning algorithms are executed in one data set\n",
    "    data   = dataset['input'].values\n",
    "    labels = dataset['label'].values\n",
    "    \n",
    "    processedData  = encodeData(data)\n",
    "    processedLabel = encodeLabel(labels)\n",
    "    \n",
    "    return processedData, processedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeData(data):\n",
    "    \n",
    "    processedData = []\n",
    "    \n",
    "    for dataInstance in data:\n",
    "        \n",
    "        # number of inputs of neurons\n",
    "        processedData.append([dataInstance >> d & 1 for d in range(10)])\n",
    "    \n",
    "    return np.array(processedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def encodeLabel(labels):\n",
    "    \n",
    "    processedLabel = []\n",
    "    \n",
    "    for labelInstance in labels:\n",
    "        if(labelInstance == \"FizzBuzz\"):\n",
    "            # Fizzbuzz\n",
    "            processedLabel.append([3])\n",
    "        elif(labelInstance == \"Fizz\"):\n",
    "            # Fizz\n",
    "            processedLabel.append([1])\n",
    "        elif(labelInstance == \"Buzz\"):\n",
    "            # Buzz\n",
    "            processedLabel.append([2])\n",
    "        else:\n",
    "            # Other\n",
    "            processedLabel.append([0])\n",
    "\n",
    "    return np_utils.to_categorical(np.array(processedLabel),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_size = 10\n",
    "drop_out = 0.2\n",
    "first_dense_layer_nodes  = 256\n",
    "second_dense_layer_nodes = 4\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    # the models are used to predict and analyze the given training set and train the algorithm accordingly\n",
    "    # dense is implementation where activation is application of activation to the output elements\n",
    "    # it allows us to model layer by layer and sharing of layers or  multiple inputs and outputs are not used in this model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # dropout in ml is dropping out some of the layers in the neural network which avoids overfitting and also regularizes the network by reducing interdepency among the neurons\n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    model.add(Activation('softmax'))\n",
    "    # for multiclass classification we used softmax\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # as it is one-hot endings of multiclass classification we used cat_crossentropy\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Creating Training and Testing Datafiles</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datafiles\n",
    "createInputCSV(101,1001,'training.csv')\n",
    "createInputCSV(1,101,'testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Creating Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Run Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_data_split = 0.2\n",
    "num_epochs = 10000\n",
    "model_batch_size = 128\n",
    "tb_batch_size = 32\n",
    "early_patience = 100\n",
    "\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "# Read Dataset\n",
    "dataset = pd.read_csv('training.csv')\n",
    "\n",
    "# Process Dataset\n",
    "processedData, processedLabel = processData(dataset)\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Training and Validation Graphs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot(subplots=True, grid=True, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Testing Accuracy [Software 2.0]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeLabel(encodedLabel):\n",
    "    if encodedLabel == 0:\n",
    "        return \"Other\"\n",
    "    elif encodedLabel == 1:\n",
    "        return \"Fizz\"\n",
    "    elif encodedLabel == 2:\n",
    "        return \"Buzz\"\n",
    "    elif encodedLabel == 3:\n",
    "        return \"FizzBuzz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "testData = pd.read_csv('testing.csv')\n",
    "\n",
    "processedTestData  = encodeData(testData['input'].values)\n",
    "processedTestLabel = encodeLabel(testData['label'].values)\n",
    "predictedTestLabel = []\n",
    "\n",
    "for i,j in zip(processedTestData,processedTestLabel):\n",
    "    y = model.predict(np.array(i).reshape(-1,10))\n",
    "    predictedTestLabel.append(decodeLabel(y.argmax()))\n",
    "    \n",
    "    if j.argmax() == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))\n",
    "\n",
    "# Please input your UBID and personNumber \n",
    "testDataInput = testData['input'].tolist()\n",
    "testDataLabel = testData['label'].tolist()\n",
    "\n",
    "testDataInput.insert(0, \"UBID\")\n",
    "testDataLabel.insert(0, \"lakshmic\")\n",
    "\n",
    "testDataInput.insert(1, \"personNumber\")\n",
    "testDataLabel.insert(1, \"50290974\")\n",
    "\n",
    "predictedTestLabel.insert(0, \"\")\n",
    "predictedTestLabel.insert(1, \"\")\n",
    "\n",
    "output = {}\n",
    "output[\"input\"] = testDataInput\n",
    "output[\"label\"] = testDataLabel\n",
    "\n",
    "output[\"predicted_label\"] = predictedTestLabel\n",
    "\n",
    "opdf = pd.DataFrame(output)\n",
    "opdf.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
